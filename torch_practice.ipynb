{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method type of Tensor object at 0x000002CBE6F96270>\n",
      "torch.Size([2, 2, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.7042, -2.1602],\n",
       "          [-1.4712,  0.2900]],\n",
       "\n",
       "         [[-0.3257,  0.4281],\n",
       "          [ 0.3159,  0.2721]]],\n",
       "\n",
       "\n",
       "        [[[-0.7300, -0.9016],\n",
       "          [-2.3673, -0.4627]],\n",
       "\n",
       "         [[-1.3425,  0.6196],\n",
       "          [-0.4664,  1.3314]]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a 4d tensor and print it.\n",
    "x = torch.randn(2,2,2,2, dtype=torch.float32)\n",
    "print(x.type)\n",
    "print(x.size())\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a tensor from a list.\n",
    "y = torch.tensor([2,3])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addition in pytorch:\n",
    "# \"z = x + y\" \"z = torch.add(x,y)\" will add element-wise\n",
    "# \"y.add_(x)\" will perform addition in-place. In torch _ indicated in-place operation.\n",
    "\n",
    "#Similarly sub, mul and div can be done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.3257,  0.4281],\n",
      "         [ 0.3159,  0.2721]],\n",
      "\n",
      "        [[-1.3425,  0.6196],\n",
      "         [-0.4664,  1.3314]]])\n",
      "tensor([[-1.3425,  0.6196],\n",
      "        [-0.4664,  1.3314]])\n"
     ]
    }
   ],
   "source": [
    "#Sllicing of tensors:\n",
    "print(x[:,1]) # Print all rows of column 1\n",
    "print(x[1,1]) # Print element at 1,1. When it holds only one item, we can use .item() to get the exact value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7042, -2.1602, -1.4712,  0.2900, -0.3257,  0.4281,  0.3159,  0.2721],\n",
       "        [-0.7300, -0.9016, -2.3673, -0.4627, -1.3425,  0.6196, -0.4664,  1.3314]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Resizing a tensor\n",
    "x_new = x.view(-1,8) #When we give -1, the second dimension is automatically figured out\n",
    "print(x_new.shape)\n",
    "x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[[[[ 1.7041695  -1.1601868 ]\n",
      "   [-0.47115052  1.290018  ]]\n",
      "\n",
      "  [[ 0.6743277   1.4280651 ]\n",
      "   [ 1.3159142   1.2720963 ]]]\n",
      "\n",
      "\n",
      " [[[ 0.2699811   0.09837598]\n",
      "   [-1.367326    0.53727686]]\n",
      "\n",
      "  [[-0.34254873  1.6195933 ]\n",
      "   [ 0.5335721   2.3314047 ]]]]\n",
      "tensor([[[[ 1.7042, -1.1602],\n",
      "          [-0.4712,  1.2900]],\n",
      "\n",
      "         [[ 0.6743,  1.4281],\n",
      "          [ 1.3159,  1.2721]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2700,  0.0984],\n",
      "          [-1.3673,  0.5373]],\n",
      "\n",
      "         [[-0.3425,  1.6196],\n",
      "          [ 0.5336,  2.3314]]]])\n"
     ]
    }
   ],
   "source": [
    "#Numpy to tensor and vice-versa:\n",
    "import numpy as np\n",
    "\n",
    "x_np = x.numpy() #Only allowed in CPU not on GPU\n",
    "print(type(x_np))\n",
    "\n",
    "#Be careful if both are in cpu memory, because if one is modiefied other will also get changes. As both are pointing to same memory.\n",
    "x.add_(1)\n",
    "print(x_np)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "x_back_to_torch = torch.from_numpy(x_np)\n",
    "print(type(x_back_to_torch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.9816, -0.7457, -0.2961], requires_grad=True)\n",
      "tensor([1.0184, 1.2543, 1.7039], grad_fn=<AddBackward0>)\n",
      "tensor(5.5135, grad_fn=<MeanBackward0>)\n",
      "tensor([2.0369, 2.5085, 3.4077])\n"
     ]
    }
   ],
   "source": [
    "#autograd:\n",
    "a = torch.randn(3, requires_grad=True)\n",
    "print(a)\n",
    "\n",
    "b = a+2\n",
    "print(b)\n",
    "\n",
    "c = b*b*3\n",
    "c = c.mean()\n",
    "print(c)\n",
    "\n",
    "c.backward() #Pass argument if c is not a scaler vector.\n",
    "print(a.grad)\n",
    "#print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Following are 3 ways to stop torch to track gradients:\n",
    "#a.requires_grad_(False)\n",
    "#a.detach_()\n",
    "#with torch.no_grad():\n",
    "    #Do some ops without grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weights.grad.zero_(): This will help avoid accumulation of grad in backward function while training with multiple epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n",
      "for Epoch 1 : Weight = 2.849 and loss = 13.745262\n",
      "for Epoch 11 : Weight = 2.041 and loss = 0.005612\n",
      "for Epoch 21 : Weight = 2.019 and loss = 0.000566\n",
      "for Epoch 31 : Weight = 2.014 and loss = 0.000308\n",
      "for Epoch 41 : Weight = 2.010 and loss = 0.000167\n",
      "for Epoch 51 : Weight = 2.008 and loss = 0.000091\n",
      "for Epoch 61 : Weight = 2.006 and loss = 0.000050\n",
      "for Epoch 71 : Weight = 2.004 and loss = 0.000027\n",
      "for Epoch 81 : Weight = 2.003 and loss = 0.000015\n",
      "for Epoch 91 : Weight = 2.002 and loss = 0.000008\n",
      "Prediction = 10.004\n"
     ]
    }
   ],
   "source": [
    "#Example model with pytorch functions:\n",
    "import torch.nn as nn\n",
    "\n",
    "EPOCHS = 100\n",
    "LR = 0.1\n",
    "\n",
    "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
    "Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
    "\n",
    "X_test = torch.tensor([5], dtype=torch.float32)\n",
    "\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "in_shape = out_shape = n_features\n",
    "print(in_shape, out_shape)\n",
    "\n",
    "\n",
    "class LinReg (nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinReg, self).__init__()\n",
    "        self.lin = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.lin(x)\n",
    "\n",
    "\n",
    "model = LinReg(in_shape, out_shape)\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "\n",
    "#Training\n",
    "for i in range(EPOCHS):\n",
    "    \n",
    "    y_pred = model(X)\n",
    "    \n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    l.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        [w,b] = model.parameters()\n",
    "        print(f\"for Epoch {i+1} : Weight = {w[0][0].item():.3f} and loss = {l:8f}\")\n",
    "\n",
    "#Test after training:\n",
    "print(f\"Prediction = {model(X_test).item():.3f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset and Dataloader:\n",
    "#from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.softmax(input, dims=0) #for applying softmax on first row\n",
    "\n",
    "# nn.CrossEntropyLoss() #for CELoss. We must be careful while using this.\n",
    "# It has already applies softmax() with itself, so we should use it only when we have raw logits in last layer, no softmax.\n",
    "# Also we need to use one hot encoding with it. \n",
    "# In case of just one output problem(Dog or not), we can use sigmoid with BCELoss()#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activation functions are available in:\n",
    "#torch.nn and torch.nn.functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9913344it [00:02, 4398257.05it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29696it [00:00, 3711273.55it/s]          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1649664it [00:00, 4123134.21it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5120it [00:00, ?it/s]                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#device config:\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#hyper-parameters:\n",
    "num_epochs = 2\n",
    "batch_size = 100\n",
    "input_size = 28*28\n",
    "hidden_l_size = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "#MNIST data-set:\n",
    "train_set = torchvision.datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True )\n",
    "test_set = torchvision.datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor() )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample-size: torch.Size([100, 1, 28, 28]) and label-size: torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "#Check the samples by taking an example\n",
    "example = iter(train_loader)\n",
    "samples, labels = example.next()\n",
    "print(f\"sample-size: {samples.shape} and label-size: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot few samples\n",
    "for i in range(6):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.imshow(samples[i][0], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 1/600 : loss=2.3044\n",
      "0 : 101/600 : loss=0.3549\n",
      "0 : 201/600 : loss=0.1461\n",
      "0 : 301/600 : loss=0.1896\n",
      "0 : 401/600 : loss=0.1893\n",
      "0 : 501/600 : loss=0.0911\n",
      "1 : 1/600 : loss=0.0992\n",
      "1 : 101/600 : loss=0.2552\n",
      "1 : 201/600 : loss=0.1213\n",
      "1 : 301/600 : loss=0.1062\n",
      "1 : 401/600 : loss=0.0483\n",
      "1 : 501/600 : loss=0.0456\n"
     ]
    }
   ],
   "source": [
    "#Model\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_l_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden_l_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(hidden_l_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        return out\n",
    "\n",
    "model = NeuralNet(input_size, hidden_l_size, num_classes)\n",
    "\n",
    "#Loss and optimizer\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "#Training\n",
    "total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "\n",
    "        #Reshaping and loading on the device\n",
    "        images = images.reshape(-1,28*28)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        #Forward pass\n",
    "        pred = model(images)\n",
    "        #loss\n",
    "        l = loss(pred, labels)\n",
    "\n",
    "        #Backward-pass\n",
    "        l.backward()\n",
    "\n",
    "        #Optimize\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if i%100 == 0:\n",
    "            print(f\"{epoch} : {i+1}/{total_steps} : loss={l.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc = 96.06%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Testing\n",
    "with torch.no_grad():\n",
    "    total_test_samples = 0\n",
    "    total_correct_preds = 0\n",
    "\n",
    "    for i, (images, labels) in enumerate(test_loader):\n",
    "        #Reshaping and loading on the device\n",
    "        images = images.reshape(-1,28*28)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        preds = model(images)\n",
    "\n",
    "        #value, index\n",
    "        _, pred_labels = torch.max(preds, 1)\n",
    "        total_test_samples += images.shape[0]\n",
    "        total_correct_preds += (pred_labels == labels).sum().item()\n",
    "    \n",
    "    acc = total_correct_preds/total_test_samples * 100\n",
    "    print(f\"acc = {acc}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "21e725f0765fabd62c8620bbbf92ef947d48bcd7feb408ca2ff19b1cab4df46b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
